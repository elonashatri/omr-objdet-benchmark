module load miniconda/4.7.12
module load cuda/12.1  # Using CUDA 12.1 for compatibility with recent PyTorch
conda activate omr_benchmark



# Load miniconda module
module load miniconda/4.7.12

# Create a new environment for OMR experiments
conda create -n omr_benchmark python=3.10 -y


training faster r-cnn

nohup python /homes/es314/omr-objdet-benchmark/scripts/faster_rccn_training.py \
    --data_dir data/images \
    --annotation_dir data/annotations \
    --class_mapping data/class_mapping.json \
    --output_dir results/faster_rcnn \
    --pretrained \
    --batch_size 1 \
    --epochs 30 \
    --optimizer adamw \
    --lr 0.0001 \
    --clip_grad_norm 1.0 \
    --val_ratio 0.2 \
    --gpu_id 1



python scripts/yolo_training.py \
    --data_dir data/images \
    --annotation_dir data/annotations \
    --class_mapping data/class_mapping.json \
    --analyze_only




python scripts/detect_notation.py \
    --model /homes/es314/omr-objdet-benchmark/results/yolov8/yolo_train/weights/best.pt \
    --image /homes/es314/omr-objdet-benchmark/data/test/splash_20231016T185758.png \
    --class_mapping data/class_mapping.json \
    --output_dir results/detections \
    --conf 0.25 \
    --max_detections 300




    # DETR 

    nohup python scripts/detr_training.py \
    --data_dir data/images \
    --train_annotation_dir data/annotations \
    --val_annotation_dir data/val_annotations \
    --class_mapping data/class_mapping.json \
    --output_dir results/detr \
    --batch_size 4 \
    --epochs 100 \
    --lr 1e-4 \
    --world_size 1 > detr-training.txt



    first run minimal_yolo_convereter then simple_yolo_train

python old_doremi_minimal_yolo_converter.py \
  --data_dir /homes/es314/DOREMI/data/old_data_72_classes/images \
  --annotation_dir /homes/es314/DOREMI/data/old_data_72_classes/page_annotations \
  --output_dir /homes/es314/omr-objdet-benchmark/old_doremi_72 \
  --class_mapping /homes/es314/DOREMI/data/old_data_72_classes/train_validation_test_records/mapping.json


nohup python 72_doremi_simple_yolo_train_8vx.py     --yaml /homes/es314/omr-objdet-benchmark/old_doremi_72/dataset.yaml     --model yolov8x.pt     --epochs 200     --batch 4     --device 1 > 72_doremi_simple_yolo_train_8vx.txt



inference:
/homes/es314/omr-objdet-benchmark/scripts/detect_notation.py

python scripts/detect_notation.py \
    --model //homes/es314/runs/staffline_extreme/weights/best.pt \
    --image /homes/es314/omr-objdet-benchmark/results/detections/Accidentals-004.png \
    --class_mapping /homes/es314/omr-objdet-benchmark/data/class_mapping.json \
    --output_dir results/detections \
    --conf 0.25 \
    --max_detections 300


    staffline focus:
    
    nohup python /homes/es314/omr-objdet-benchmark/scripts/staffline-recognition/yolo-converter.py \
    --yaml results/yolo_fixed/dataset.yaml \
    --model yolov8x.pt \
    --epochs 200 \
    --batch 4 \
    --device 2 \
    --staffline_class_id 2 > staffline-focus.txt



    inference on staffline focused model
    /homes/es314/omr-objdet-benchmark/runs/train/phase12/weights/best.pt

    python scripts/detect_notation.py \
    --model /homes/es314/omr-objdet-benchmark/runs/train/phase12/weights/best.pt \
    --image /homes/es314/omr-objdet-benchmark/2-solo-Christmas_Greeting-003.png \
    --class_mapping /homes/es314/omr-objdet-benchmark/data/class_mapping.json \
    --output_dir results/detections \
    --conf 0.25 \
    --max_detections 300




tensorboard --logdir /homes/es314/runs



encoding stuff:
after runnning inference we have two files with bounding boxes, csv and json. then we apply 


python /homes/es314/omr-objdet-benchmark/scripts/encoding/link_obj.py --detection-file /homes/es314/omr-objdet-benchmark/results/detections/Accidentals-004_detections.json --output-dir /homes/es314/omr-objdet-benchmark/scripts/encoding

the updated one with a full pipeline:
python /homes/es314/omr-objdet-benchmark/scripts/encoding/complete_pipeline.py --image /homes/es314/omr-objdet-benchmark/scripts/encoding/stave-only.png --model /homes/es314/runs/staffline_extreme/weights/best.pt --class_mapping /homes/es314/omr-objdet-benchmark/data/class_mapping.json


to visualise
python /homes/es314/omr-objdet-benchmark/scripts/encoding/visualisation.py --image /homes/es314/omr-objdet-benchmark/scripts/encoding/results/Accidentals-004.png --data /homes/es314/omr-objdet-benchmark/scripts/encoding/results/pitch_identification_linked_data.json



and finally to run the json to midi converter:
python /homes/es314/omr-objdet-benchmark/scripts/encoding/to_midi/json_to_midi_converter.py --input /homes/es314/omr-objdet-benchmark/scripts/encoding/results/pitched_data/demisemiquavers_simple-085_pitched.json --output /homes/es314/omr-objdet-benchmark/scripts/encoding/results/pitched_data/demisemiquavers_simple-085_pitched.mid


json to musicxml

python /homes/es314/omr-objdet-benchmark/scripts/encoding/to_midi/json_to_musicxml.py --input /homes/es314/omr-objdet-benchmark/scripts/encoding/results/pitched_data/stave-only_pitched.json --output /homes/es314/omr-objdet-benchmark/scripts/encoding/results/pitched_data/stave-only_pitched.musicxml --beamall


website up and runnong code:
cd  app/backend 
run the backend first:  /homes/es314/.conda/envs/omr_benchmark/bin/python app.py

~/omr-objdet-benchmark/scripts/app$ cd frontend/
~/omr-objdet-benchmark/scripts/app/frontend$ npm run dev